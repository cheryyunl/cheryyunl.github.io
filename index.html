<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YPD2Y5FXB5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YPD2Y5FXB5');
</script>
	
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yongyuan Cheryl Liang</title>
    
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <meta name="author" content="Yongyuan Liang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="Description" content="Yongyuan Liang">
    <meta name="keywords" content="Yongyuan Liang, cheryyunl, Cheryl">

    <link rel="icon" type="icon" href="favicon.png">

</head>
<body class="bg_colour">
    <table border=0 class="bg_colour" style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">
                
                <!-- Name tab -->
                <table border=0 class="bg_colour" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    
                    
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p style="text-align:center">
                                <h1  style="text-align:center"><name>Yongyuan Cheryl Liang</name></h1>
                            </p>
                </p>
                <p>My research focuses on developing foundation models and intelligent agents.
                I actively explore both theoretical frameworks and empirical findings, with specific research interests in:
                <ul>
                  <li><strong>Multi-Modal Foundation Models</strong>: Large multi-modal models/generative models for 2D/3D virtual and physical agentic tasks.</li>
                  <li><strong>Alignment</strong>: Post-training alignment including human preference alignment and cross-modality alignment.</li>
                </ul>
		In the previous few years, I have conducted research in Reinforcement Learning, Representations and Robustness.</p> 

                </p>
                        </td>
                        <td style="padding:2.5%;width:45%;max-width:40%">
				 <a href="images/myself.jpeg"><img style="width:70%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/myself.jpeg" class="hoverZoomLink"></a>
                            <!-- <a href="images/RishabKhinchaProfilePic.
"><img style="width:100%;max-width:100%" alt="profile photo" src="images/RishabKhinchaProfilePic.png" class="hoverZoomLink"></a> -->
                        </td>
                    </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:60%;vertical-align:middle">
                          <p style="text-align:center">
                            <a href="mailto:charlotte9762@gmail.com">Email</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?user=GQToORIAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                            <a href="https://github.com/cheryyunl">Github</a>&nbsp;/&nbsp;
                            <a href="https://twitter.com/cheryyun_l">Twitter</a> 
                          </p>
                        </td>
                    </tr>
                </tbody></table>
              <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr> 
                <p>I join <a href="https://www.cs.umd.edu/" target="_blank" rel="noopener noreferrer">UMD CS</a> as a PhD student, advised by <a href="https://furong-huang.com/" target="_blank" rel="noopener noreferrer"> Prof. Furong Huang</a>. 
                  I received my B.S. degree in Mathematics from Sun Yat-sen University.</p>
                  I'm always happy to collaborate with graduate/undergraduate students. Please drop me an email if you want to work with me.
                  <br>
                  I'm actively looking for <font color=#C21232>part-time/full-time internship opportunities starting from Spring 2026</font>. Feel free to reach out if you're interested in my research.</p>
              </tr></tbody></table>
                <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-news" id="news">
                  <p style="font-size:18px;color:#C21232"><b>News</b></p>
                </button>
                <!-- <div class="container"> -->
                <div id="content-news" class="collapse in">
                <!-- <div class="scroll">  -->

                <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Sept' 25 &nbsp</p>
                    </td>
                    <td>
                      One paper to appear in NeurIPS 2025.
                    </td>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Feb' 25 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://microsoft.github.io/Magma/">Magma</a> to appear in CVPR 2025. Code and models have been released.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 25 &nbsp</p>
                    </td>
                    <td>
                      Two papers to appear in ICLR 2025.</a>
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 25 &nbsp</p>
                    </td>
                    <td>
                      Start to update <a href="https://github.com/cheryyunl/awesome-generalist-agents">Awesome-Generalist-Agents</a>.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Sept' 24 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://cheryyunl.github.io/make-an-agent/">Make-An-Agent</a> to appear in NeurIPS 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> June' 24 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://ace-rl.github.io/">ACE</a> has been selected as a <font color=#C21232>long oral presentation</font> in ICML 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> May' 24 &nbsp</p>
                    </td>
                    <td>
                      Two papers to appear in ICML 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Feb' 24 &nbsp</p>
                    </td>
                    <td>
                      Awarded a Dean's Fellowship.
                    </td>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 24 &nbsp</p>
                    </td>
                    <td>
                      Three papers to appear in ICLR 2024, including two spotlights and one poster.
                    </td>
                  </tr>
                   
                </tbody></table>
                <!-- </div> -->
                <!-- </div> -->
                </div>
                <br>
                <br>

<!-- Research -->
<heading>Publications</heading>

<div class="filter-section">
    <strong>By recency:</strong> 
    <span class="filter-link active" data-mode="selected"><strong>Show Selected</strong></span> / 
    <strong>Show All by Topics:</strong>  
    <span class="filter-link" data-topic="foundation">Foundation Model</span> / 
    <span class="filter-link" data-topic="alignment">Alignment</span> /
    <span class="filter-link" data-topic="rl">Reinforcement Learning</span> / 
    <span class="filter-link" data-topic="representation">Representations</span> / <br>
    <span class="filter-link" data-topic="trustworthy">Trustworthy AI</span>.
</div>

<p></p>
<p style="color:#3d3f41"><strong>* denotes Equal Contributions and Project Lead; &dagger; indicates Equal Advising.</strong></p>

<table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- === ALIGNMENT PAPERS === -->
<!-- Avocado Paper -->
<tr class="publication-row" data-category="alignment" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/avocado_icon.jpg' width="80">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">Multi-Objective Alignment</div>
  <p>
  <a href="">
  <papertitle>Avocado: A Unified Contrastive Framework for Multi-Objective Alignment of Language Models</papertitle>
  </a>
  <br>
  <strong>Yongyuan Liang*</strong>, Xuejun Zhang*, Ziqiao Ma*, Joyce Chai, Furong Huang
  <br>				
  <br>
  <em>arXiv</em>, 2025
  <br>
  <a href="" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="" class="special-link">Code</a>&nbsp/&nbsp
  <a href="" class="special-link">Models & Datasets</a>&nbsp/&nbsp
  <a href="" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- === FOUNDATION MODEL PAPERS === -->
<!-- LEMON Paper -->
<tr class="publication-row" data-category="foundation" data-selected="true">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/lemon.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">3D Large Multi-Modal Model</div>
  <p>
  <a href="">
  <papertitle>LEMON: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</papertitle>
  </a>
  <br>
  <strong>Yongyuan Liang</strong>, Xiyao Wang, Yuanchen Ju, Jianwei Yang, Furong Huang
  <br>				
  <br>
  <em>arXiv</em>, 2025<br>
  <font color=#C21232><strong>Spotlight Talks</strong></font> at <em>CVPR Workshop CVinW</em>, 2025
  <br>
  <a href="" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="" class="special-link">Code</a>&nbsp/&nbsp
  <a href="" class="special-link">Models & Datasets</a>&nbsp/&nbsp
  <a href="" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- Magma Paper -->
<tr class="publication-row" data-category="foundation" data-selected="true">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/magma.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">Agentic Large Multi-Modal Model</div>
  <p>
  <a href="https://microsoft.github.io/Magma">
  <papertitle>Magma: A Foundation Model for Multimodal AI Agents</papertitle>
  </a>
  <br>
  Magma Team
  <br>				
  <br>
  <em><strong>CVPR</strong></em>, 2025
  <br>
  <a href="https://microsoft.github.io/Magma" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://www.arxiv.org/abs/2502.13130" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/microsoft/Magma" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://huggingface.co/microsoft/Magma-8B" class="special-link">Models & Datasets</a>&nbsp/&nbsp
  <a href="https://x.com/cheryyun_l/status/1892354049988591748" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- TraceVLA Paper -->
<tr class="publication-row" data-category="foundation" data-selected="true">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/tracevla.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">Embodied Large Multi-Modal Model</div>
  <p>
  <a href="https://tracevla.github.io/">
  <papertitle>TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</papertitle>
  </a>
  <br>
  Ruijie Zheng*, <strong>Yongyuan Liang*</strong>, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, Jianwei Yang 
  <br>				
  <br>
  <em><strong>ICLR</strong></em>, 2025<br>
  <font color=#C21232><strong>Oral Talks</strong></font> at <em>ICLR Workshop GenBot</em>, 2025
  <br>
  <a href="https://tracevla.github.io/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2412.10345" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/FrankZheng2022/tracevla" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://huggingface.co/collections/furonghuang-lab/tracevla-677d98483d23e9cec903d076" class="special-link">Models</a>&nbsp/&nbsp
  <a href="https://x.com/cheryyun_l/status/1877746315297230874" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- Make-An-Agent Paper -->
<tr class="publication-row" data-category="foundation" data-selected="true">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/agent.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">Generative Model</div>
  <p>
  <a href="https://cheryyunl.github.io/make-an-agent/">
  <papertitle>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</papertitle>
  </a>
  <br>
  <strong>Yongyuan Liang</strong>, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu
  <br>				
  <br>
  <em><strong>NeurIPS</strong></em>, 2024<br>
  <font color=#C21232><strong>Oral Talks</strong></font> at <em>NeurIPS Workshop AFM</em>, 2024
  <br>
  <a href="https://cheryyunl.github.io/make-an-agent/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/pdf/2407.10973" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/cheryyunl/Make-An-Agent" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://huggingface.co/cheryyunl/Make-An-Agent" class="special-link">Models & Dataset</a>&nbsp/&nbsp
  <a href="https://x.com/cheryyun_l/status/1813226234979184687" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- ViCrit Paper -->
<tr class="publication-row" data-category="foundation" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/vicrit.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="">
  <papertitle>ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs</papertitle>
  </a>
  <br>
  Xiyao Wang*, Zhengyuan Yang*, Chao Feng*, <strong>Yongyuan Liang</strong>, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li&dagger;, Furong Huang&dagger;, Lijuan Wang&dagger;
  <br>				
  <br>
  <em><strong>NeurIPS</strong></em>, 2025
  <br>
  <a href="" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2504.07934" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/si0wang/ViCrit" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://huggingface.co/collections/russwang/ViCrit" class="special-link">Models & Datasets</a>&nbsp/&nbsp
  <a href="" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- === REPRESENTATIONS PAPERS === -->
<!-- MCR Paper -->
<tr class="publication-row" data-category="representation" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/mcr.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://robots-pretrain-robots.github.io/">
  <papertitle>Robots Pre-Train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets</papertitle>
  </a>
  <br>
  Guangqi Jiang*, Yifei Sun*, Tao Huang*, Huanyu Li, <strong>Yongyuan Liang</strong>&dagger;, Huazhe Xu&dagger;
  <br>				
  <br>
  <em><strong>ICLR</strong></em>, 2025
  <br>
  <a href="https://robots-pretrain-robots.github.io/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2410.22325" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/luccachiang/robots-pretrain-robots" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://huggingface.co/GqJiang/robots-pretrain-robots" class="special-link">Models</a>&nbsp/&nbsp
  <a href="https://x.com/LuccaChiang/status/1851651164187635732" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- PREMIER-TACO Paper -->
<tr class="publication-row" data-category="representation" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/premier-taco.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://premiertaco.github.io/">
  <papertitle>PREMIER-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss</papertitle>
  </a>
  <br>
  Ruijie Zheng, <strong>Yongyuan Liang</strong>, Xiyao Wang, Shuang Ma, Hal Daumé III, Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Basu, Furong Huang
  <br>
  <br>
  <em><strong>ICML</strong></em>, 2024
  <br>
  <a href="https://premiertaco.github.io/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2402.06187" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/FrankZheng2022/premier_taco" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://x.com/furongh/status/1757393309637423450?s=20" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- === REINFORCEMENT LEARNING PAPERS === -->
<!-- ACE Paper -->
<tr class="publication-row" data-category="rl" data-selected="true">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/ace.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <div class="category-tag category-tag-selected">Reinforcement Learning</div>
  <p>
  <a href="https://ace-rl.github.io/">
  <papertitle>ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization</papertitle>
  </a>
  <br>
  Tianying Ji*, <strong>Yongyuan Liang*</strong>, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, Huazhe Xu
  <br>				
  <br>
  <em><strong>ICML</strong></em>, 2024 <font color=#C21232><strong>(Oral - Top 1.5%)</strong></font>
  <br>
  <a href="https://ace-rl.github.io/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2402.14528" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/jity16/ACE-Off-Policy-Actor-Critic-with-Causality-Aware-Entropy-Regularization" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://x.com/cheryyun_l/status/1790374414187409440" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- DrM Paper -->
<tr class="publication-row" data-category="rl" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/drm.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://drm-rl.github.io/">
  <papertitle>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</papertitle>
  </a>
  <br>
  Guowei Xu*, Ruijie Zheng*, <strong>Yongyuan Liang*</strong>,
  Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu
  <br>
  <br>
  <em><strong>ICLR</strong></em>, 2024 <font color=#C21232><strong>(Spotlight - Top 5%)</strong></font>
  <br>
  <a href="https://drm-rl.github.io/" class="special-link">Project Page</a> &nbsp/&nbsp
  <a href="https://arxiv.org/abs/2310.19668" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/XuGW-Kevin/DrM" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://twitter.com/ruijie_zheng12/status/1720097914889064471?s=20" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- === TRUSTWORTHY AI PAPERS === -->
<!-- WocaR Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/wocar.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="">
  <papertitle>Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning</papertitle>
  </a>
  <br>
  <strong>Yongyuan Liang*</strong>, Yanchao Sun*, Ruijie Zheng, Furong Huang
  <br>
  <br>
  <em><strong>NeurIPS</strong></em>, 2022<br>
   <font color=#C21232><strong>Spotlight Talks</strong></font> at <em>NeurIPS Workshop SafeRL</em>, 2021
  <br>
  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8d6b1d775014eff18256abeb207202ad-Paper-Conference.pdf" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/umd-huang-lab/WocaR-RL" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://neurips.cc/media/neurips-2022/Slides/54214.pdf" class="special-link">Slides</a>
  </p>
  </td>
</tr>

<!-- Game Theoretic Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/grad.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="">
  <papertitle>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</papertitle>
  </a>
  <br>
  <strong>Yongyuan Liang</strong>, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Benjamin Eysenbach, Tuomas Sandholm, Furong Huang, Stephen Marcus McAleer
  <br>
  <br>
  <em><strong>ICLR</strong></em>, 2024
  <br>
  <a href="https://openreview.net/pdf?id=wZWTHU7AsQ" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://x.com/cheryyun_l/status/1787478879764099140" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- Backdoor Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/backdoor.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://openreview.net/attachment?id=QTviVh3VQU&name=pdf">
  <papertitle>Is poisoning a real threat to LLM alignment? Maybe more so than you think</papertitle>
  </a>
  <br>
  Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, <strong>Yongyuan Liang</strong>, Furong Huang
  <br>				
  <br>
  <em><strong>AAAI</strong></em>, 2025<br>
  <em>ICML Workshop on Models of Human Feedback for AI Alignment</em>, 2024
  <br>
  <a href="https://arxiv.org/pdf/2406.12091" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="https://github.com/pankayaraj/AAAI_2025_RLHFPoisoning" class="special-link">Code</a>
  </p>
  </td>
</tr>

<!-- Beyond Worst Case Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/beyond.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://protected-beyond-worst-case.github.io">
  <papertitle>Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies</papertitle>
  </a>
  <br>
  Xiangyu Liu*, Chenghao Deng*, Yanchao Sun, <strong>Yongyuan Liang</strong>, Furong Huang
  <br>
  <br>
  <em><strong>ICLR</strong></em>, 2024 <font color=#C21232><strong>(Spotlight - Top 5%)</strong></font>
  <br>
  <a href="https://protected-beyond-worst-case.github.io/home/" class="special-link">Project Page</a>&nbsp/&nbsp
  <a href="https://openreview.net/pdf?id=DFTHW0MyiW" class="special-link">Paper</a>&nbsp/&nbsp
  <a href="https://github.com/umd-huang-lab/PROTECTED" class="special-link">Code</a>&nbsp/&nbsp
  <a href="https://x.com/furongh/status/1760859987851591681?s=20" class="special-link">Twitter</a>
  </p>
  </td>
</tr>

<!-- PAAD Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
  <img src='images/paad.gif' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
  <p>
  <a href="https://ycsun2017.github.io/paad/index.html">
  <papertitle>Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL</papertitle>
  </a>
  <br>
  Yanchao Sun, Ruijie Zheng, <strong>Yongyuan Liang</strong>, Furong Huang
  <br>
  <br>
  <em><strong>ICLR</strong></em>, 2022<br>
  <font color=#C21232><strong>Best Paper Award</strong></font> at <em>NeurIPS Workshop SafeRL</em>, 2021
  <br>
  <a href="https://umd-huang-lab.github.io/evasion-rl/" class="special-link">Project Page</a>&nbsp/&nbsp
  <a href="https://openreview.net/pdf?id=JM2kFbJvvI" class="special-link">Paper</a>&nbsp/&nbsp
  <a href="https://github.com/umd-huang-lab/paad_adv_rl" class="special-link">Code</a>
  </p>
  </td>
</tr>

<!-- CMARL Paper -->
<tr class="publication-row" data-category="trustworthy" data-selected="false">
  <td class="tdimg" style="padding:20px;width:25%;vertical-align:center">
  <img src='images/cmarl.png' width="230">
  </td>
  <td class="tdcontent" style="padding:20px;width:75%;vertical-align:center">
  <p>
  <a href="https://openreview.net/forum?id=dCOL0inGl3e">
  <papertitle>Certifiably Robust Policy Learning against Adversarial Communication in Multi-agent Systems</papertitle>
  </a>
  <br>
  Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, <strong>Yongyuan Liang</strong>, Soheil Feizi, Sumitra Ganesh, Furong Huang
  <br>
  <br>
  <em><strong>ICLR</strong></em>, 2023
  <br>
  <a href="https://openreview.net/pdf?id=dCOL0inGl3e" class="special-link">Paper</a> &nbsp/&nbsp
  <a href="" class="special-link">Code</a>
  </p>
  </td>
</tr>

</tbody></table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const filterLinks = document.querySelectorAll('.filter-link');
    const publicationRows = document.querySelectorAll('.publication-row');
    const categoryTags = document.querySelectorAll('.category-tag-selected');
    
    // Initial：Show Selected
    showSelectedMode();
    
    filterLinks.forEach(link => {
        link.addEventListener('click', function() {
            const mode = this.getAttribute('data-mode');
            const topic = this.getAttribute('data-topic');
            
            // update active
            filterLinks.forEach(l => l.classList.remove('active'));
            this.classList.add('active');
            
            if (mode === 'selected') {
                showSelectedMode();
            } else if (topic) {
                showTopicMode(topic);
            }
        });
    });
    
    function showSelectedMode() {
        // show category tags
        categoryTags.forEach(tag => {
            tag.style.display = 'inline-block';
        });
        
        // show selected
        publicationRows.forEach(row => {
            const isSelected = row.getAttribute('data-selected') === 'true';
            row.classList.toggle('hidden', !isSelected);
        });
    }
    
    function showTopicMode(topic) {
        // hide category tags
        categoryTags.forEach(tag => {
            tag.style.display = 'none';
        });
        
        publicationRows.forEach(row => {
            const category = row.getAttribute('data-category');
            row.classList.toggle('hidden', category !== topic);
        });
    }
});
</script>

<br>
<heading>Professional Service</heading>
<table style="padding:0px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr>
    <td>
      <p><strong>Conference Program Committee:</strong> ICML(2022, 2023, 2024, 2025), NeurIPS(2021, 2022, 2023, 2024, 2025), ICLR(2021, 2022, 2023, 2024, 2025)</p>
    </td>
  </tr>
  <tr>
    <td>
      <p><strong>Workshop Program Committee:</strong> <a href="https://sites.google.com/view/fmdm-neurips23/">FMDM at NeurIPS 2023</a>, <a href="https://bialign-workshop.github.io/">Bi-Align at ICLR 2025</a>, <a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">CVinW at CVPR 2025</a></p>
    </td>
  </tr>
  <br>
  <br>
</tbody></table>
<br>

<br>
<p></p>
<misc><b>Misc</b></misc>
<br>
<br>
<table style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <p>If my name is a bit tricky to pronounce for you, it is also great to call me Cheryl [ˈʃerəl].</p>
  <p>I've been playing the violin🎻 for over 15 years and served as a principal violinist in the university orchestra.</p>
  <p>I used to play the piano as a classical music enthusiast and held ABRSM Grade 8 Piano certification.</p>
  <p>Been a fan of Novak Djokovic since 2012.</p>
  <p>My Erdős number = <a href="https://www.csauthors.net/distance/paul-erdos/yongyuan-liang">4</a>.</p>
  </tr>
</tbody></table>
                
<br>
<br>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <br>
      <p style="text-align:right;font-size:small;color:#a0a7b38b">&copy; Yongyuan Liang<br><a style="font-size:small;color:#a0a7b38b" href="https://github.com/jonbarron/website">credits</a></font></p>
    </td>
  </tr>
</tbody></table>

</body>

</html>
