<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YPD2Y5FXB5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YPD2Y5FXB5');
</script>
	
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yongyuan Liang</title>
    
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <meta name="author" content="Yongyuan Liang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="Description" content="Yongyuan Liang">
    <meta name="keywords" content="Yongyuan Liang, cheryyunl, Cheryl">

    <link rel="icon" type="icon" href="favicon.png">

</head>
<body class="bg_colour">
    <table border=0 class="bg_colour" style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">
                
                <!-- Name tab -->
                <table border=0 class="bg_colour" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    
                    
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p style="text-align:center">
                                <h1  style="text-align:center"><name>Yongyuan Liang</name></h1>
                            </p>
                </p>
                <p>Yongyuan (Cheryl) Liang's research focuses on developing foundation models and intelligent agents.
                She actively explores both theoretical frameworks and empirical findings, with specific research interests in:
                <ul>
                  <li><strong>Multi-modal Foundation models</strong>: Large multi-modal models/generative models for 2D/3D virtual and physical agentic tasks.</li>
                  <li><strong>Alignment</strong>: Post-training alignment including human preference alignment and cross-modality alignment.</li>
                </ul>

                </p>
                        </td>
                        <td style="padding:2.5%;width:45%;max-width:40%">
				 <a href="images/myself.jpeg"><img style="width:70%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/myself.jpeg" class="hoverZoomLink"></a>
                            <!-- <a href="images/RishabKhinchaProfilePic.
"><img style="width:100%;max-width:100%" alt="profile photo" src="images/RishabKhinchaProfilePic.png" class="hoverZoomLink"></a> -->
                        </td>
                    </tr>
                </tbody></table>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:60%;vertical-align:middle">
                          <p style="text-align:center">
                            <a href="mailto:charlotte9762@gmail.com">Email</a> &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?user=GQToORIAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                            <a href="https://github.com/cheryyunl">Github</a>&nbsp;/&nbsp;
                            <a href="https://twitter.com/cheryyun_l">Twitter</a> 
                          </p>
                        </td>
                    </tr>
                </tbody></table>
              <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr> 
                <p>I join <a href="https://www.cs.umd.edu/" target="_blank" rel="noopener noreferrer">UMD CS</a> as a PhD student, advised by <a href="https://furong-huang.com/" target="_blank" rel="noopener noreferrer"> Prof. Furong Huang</a>. 
                  I received my B.S. degree in Mathematics from Sun Yat-sen University.</p>
                  I'm always happy to collaborate with graduate/undergraduate students. Please drop me an email if you want to work with me.
                  <br>
                  I'm actively looking for <font color=#C21232>part-time/full-time internship opportunities starting from Fall 2025</font>. Feel free to reach out if you're interested in my research.</p>
              </tr></tbody></table>
                <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-news" id="news">
                  <p style="font-size:18px;color:#C21232"><b>News</b></p>
                </button>
                <!-- <div class="container"> -->
                <div id="content-news" class="collapse in">
                <!-- <div class="scroll">  -->

                <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Feb' 25 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://microsoft.github.io/Magma/">Magma</a> to appear in CVPR 2025. Code and models have been released.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 25 &nbsp</p>
                    </td>
                    <td>
                      Two papers to appear in ICLR 2025.</a>
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 25 &nbsp</p>
                    </td>
                    <td>
                      Start to update <a href="https://github.com/cheryyunl/awesome-generalist-agents">Awesome-Generalist-Agents</a>.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Sept' 24 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://cheryyunl.github.io/make-an-agent/">Make-An-Agent</a> to appear in NeurIPS 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> June' 24 &nbsp</p>
                    </td>
                    <td>
                      <a href="https://ace-rl.github.io/">ACE</a> has been selected as a <font color=#C21232>long oral presentation</font> in ICML 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> May' 24 &nbsp</p>
                    </td>
                    <td>
                      Two papers to appear in ICML 2024.
                    </td>
                  </tr>
                  <tr>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Feb' 24 &nbsp</p>
                    </td>
                    <td>
                      Awarded a Dean's Fellowship.
                    </td>
                  </tr>
                  <tr>
                    <td>
                        <p style="color:#5b9edd; display:inline"> Jan' 24 &nbsp</p>
                    </td>
                    <td>
                      Three papers to appear in ICLR 2024, including two spotlights and one poster.
                    </td>
                  </tr>
                   
                </tbody></table>
                <!-- </div> -->
                <!-- </div> -->
                </div>
                <br>
                <br>

<!-- Research -->

                <heading>Selected Publications & Preprints</heading>
                <p></p>
                <p style="color:#3d3f41"><strong>* denotes <span class="highlight">Equal Contributions and Project Lead</span>; &dagger; indicates Equal Advising.</strong></p>
                <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="width:30%;vertical-align:center">
                      <h4>Foundation Model</h4>
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center"></td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/avocado_icon.jpg' width="80">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="">
                  <papertitle>Avocado🥑: A Unified Contrastive Framework for Multi-Objective Alignment of Language Models</papertitle>
                  </a>
                  <br>
                  <strong>Yongyuan Liang*</strong>, Xuejun Zhang*, Ziqiao Ma*, Joyce Chai, Furong Huang
                  <br>				
                  <br>
                  <em>arXiv</em>, 2025</div>
                  <br>
                  <br>
                  <a href="">Project Page</a> &nbsp/&nbsp
                  <a href="">Paper</a> &nbsp/&nbsp
                  <a href="">Code</a>&nbsp/&nbsp
                  <a href="">Models & Datasets</a>&nbsp/&nbsp
                  <a href="">Twitter</a>
                  </p>
                  </td>
                </tr>
                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/lemon_icon.png' width="80">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="">
                  <papertitle>LEMON🍋: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</papertitle>
                  </a>
                  <br>
                  <strong>Yongyuan Liang</strong>, Xiyao Wang, Yuanchen Ju, Jianwei Yang, Furong Huang
                  <br>				
                  <br>
                  <em>Coming soon</em>, 2025</div>
                  <br>
                  <br>
                  <a href="">Project Page</a> &nbsp/&nbsp
                  <a href="">Paper</a> &nbsp/&nbsp
                  <a href="">Code</a>&nbsp/&nbsp
                  <a href="">Models & Datasets</a>&nbsp/&nbsp
                  <a href="">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/magma.gif' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://microsoft.github.io/Magma">
                  <papertitle>Magma: A Foundation Model for Multimodal AI Agents</papertitle>
                  </a>
                  <br>
                  Magma Team
                  <br>				
                  <br>
                  <em><strong>CVPR</strong></em>, 2025</div>
                  <br>
                  <br>
                  <a href="https://microsoft.github.io/Magma">Project Page</a> &nbsp/&nbsp
                  <a href="https://www.arxiv.org/abs/2502.13130">Paper</a> &nbsp/&nbsp
                  <a href="https://github.com/microsoft/Magma">Code</a>&nbsp/&nbsp
                  <a href="https://huggingface.co/microsoft/Magma-8B">Models & Datasets</a>&nbsp/&nbsp
                  <a href="https://x.com/cheryyun_l/status/1892354049988591748">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/tracevla.gif' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://tracevla.github.io/">
                  <papertitle>TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</papertitle>
                  </a>
                  <br>
                  Ruijie Zheng*, <strong>Yongyuan Liang*</strong>, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, Jianwei Yang 
                  <br>				
                  <br>
                  <em><strong>ICLR</strong></em>, 2025</div><br>
                  <em>ICLR Workshop GenBot</em>, 2025<font color=#C21232><strong>(Oral Talks)</strong></font></div>
                  <br>
                  <br>
                  <a href="https://tracevla.github.io/">Project Page</a> &nbsp/&nbsp
                  <a href="https://arxiv.org/abs/2412.10345">Paper</a> &nbsp/&nbsp
                  <a href="https://github.com/FrankZheng2022/tracevla">Code</a>&nbsp/&nbsp
                  <a href="https://huggingface.co/collections/furonghuang-lab/tracevla-677d98483d23e9cec903d076">Models</a>&nbsp/&nbsp
                  <a href="https://x.com/cheryyun_l/status/1877746315297230874">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/agent.gif' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://cheryyunl.github.io/make-an-agent/">
                  <papertitle>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</papertitle>
                  </a>
                  <br>
                  <strong>Yongyuan Liang</strong>, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu
                  <br>				
                  <br>
                  <em><strong>NeurIPS</strong></em>, 2024</div>
                  <br>
                  <em>NeurIPS Workshop AFM</em>, 2024 <font color=#C21232><strong>(Oral Talks - Top 3%)</strong></font></div>
                  <br>
                  <a href="https://cheryyunl.github.io/make-an-agent/">Project Page</a> &nbsp/&nbsp
                  <a href="https://arxiv.org/pdf/2407.10973">Paper</a> &nbsp/&nbsp
                  <a href="https://github.com/cheryyunl/Make-An-Agent">Code</a>&nbsp/&nbsp
                  <a href="https://huggingface.co/cheryyunl/Make-An-Agent">Models & Dataset</a>&nbsp/&nbsp
                  <a href="https://x.com/cheryyun_l/status/1813226234979184687">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td style="width:30%;vertical-align:center">
                    <h4>Representations</h4>
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center"></td>
                </tr>
                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/premier-taco.png' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://premiertaco.github.io/">
                  <papertitle>PREMIER-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss</papertitle>
                  </a>
                  <br>
                  Ruijie Zheng, <strong>Yongyuan Liang</strong>, Xiyao Wang, Shuang Ma, Hal Daumé III, Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Basu, Furong Huang
                  <br>
                  <br>
                  <em><strong>ICML</strong></em>, 2024<br>
                  <em>NeurIPS Workshop FMDM</em>, 2023
                  <br>
                  <a href="https://premiertaco.github.io/">Project Page</a> &nbsp/&nbsp
                  <a href="https://arxiv.org/abs/2402.06187">Paper</a> &nbsp/&nbsp
                  <a href="https://github.com/FrankZheng2022/premier_taco">Code</a>&nbsp/&nbsp
                  <a href="https://x.com/furongh/status/1757393309637423450?s=20">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/mcr.gif' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://robots-pretrain-robots.github.io/">
                  <papertitle>Robots Pre-Train Robots: Manipulation- Centric Robotic Representation from Large- Scale Robot Datasets</papertitle>
                  </a>
                  <br>
                  Guangqi Jiang*, Yifei Sun*, Tao Huang*, Huanyu Li, <strong>Yongyuan Liang</strong>&dagger;, Huazhe Xu&dagger;
                  <br>				
                  <br>
                  <em><strong>ICLR</strong></em>, 2025</div>
                  <br>
                  <br>
                  <a href="https://robots-pretrain-robots.github.io/">Project Page</a> &nbsp/&nbsp
                  <a href="https://arxiv.org/abs/2410.22325">Paper</a> &nbsp/&nbsp
                  <a href="https://github.com/luccachiang/robots-pretrain-robots">Code</a>&nbsp/&nbsp
                  <a href="https://huggingface.co/GqJiang/robots-pretrain-robots">Models</a>&nbsp/&nbsp
                  <a href="https://x.com/LuccaChiang/status/1851651164187635732">Twitter</a>
                  </p>
                  </td>
                </tr>

                <tr>
                  <td style="width:30%;vertical-align:center">
                    <h4>Reinforcement Learning</h4>
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center"></td>
                </tr>
                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images/ace.png' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="https://ace-rl.github.io/">
                <papertitle>ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization</papertitle>
                </a>
                <br>
                Tianying Ji*, <strong>Yongyuan Liang*</strong>, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, Huazhe Xu
                <br>				
                <br>
                <em><strong>ICML</strong></em>, 2024 <font color=#C21232><strong>(Oral - Top 1.5%)</strong></font></div>
                <br>
                <a href="https://ace-rl.github.io/">Project Page</a> &nbsp/&nbsp
                <a href="https://arxiv.org/abs/2402.14528">Paper</a> &nbsp/&nbsp
                <a href="https://github.com/jity16/ACE-Off-Policy-Actor-Critic-with-Causality-Aware-Entropy-Regularization">Code</a>&nbsp/&nbsp
                <a href="https://x.com/cheryyun_l/status/1790374414187409440">Twitter</a>
                </p>
                </td>
                </tr>
    
                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images/drm.gif' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="https://drm-rl.github.io/">
                <papertitle>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</papertitle>
                </a>
                <br>
                Guowei Xu*, Ruijie Zheng*, <strong>Yongyuan Liang*</strong>,
                Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu
                <br>
                <br>
                <em><strong>ICLR</strong></em>, 2024 <font color=#C21232><strong>(Spotlight - Top 5%)</strong></font></div>
                <br>
                <em>CORL Workshop PRL</em>, 2023
                <br>
                <a href="https://drm-rl.github.io/">Project Page</a> &nbsp/&nbsp
                <a href="https://arxiv.org/abs/2310.19668">Paper</a> &nbsp/&nbsp
                <a href="https://github.com/XuGW-Kevin/DrM">Code</a>&nbsp/&nbsp
                <a href="https://twitter.com/ruijie_zheng12/status/1720097914889064471?s=20">Twitter</a>
                </p>
                </td>
                </tr>

                <tr>
                <td style="width:30%;vertical-align:center">
                  <h4>Trustworthy AI</h4>
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center"></td>
                </tr>
                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images/grad.gif' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="">
                <papertitle>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</papertitle>
                </a>
                <br>
                <strong>Yongyuan Liang</strong>, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Benjamin Eysenbach, Tuomas Sandholm, Furong Huang, Stephen Marcus McAleer
                <br>
                <br>
                <em><strong>ICLR</strong></em>, 2024 
                <br>
                <em>ICML Workshop AdvML-Frontiers</em>, 2023
                <br>
                <a href="https://openreview.net/pdf?id=wZWTHU7AsQ">Paper</a> &nbsp/&nbsp
                <a href="">Code</a>&nbsp/&nbsp
                <a href="https://x.com/cheryyun_l/status/1787478879764099140">Twitter</a>
                </p>
                </td>
                </tr>

                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images/wocar.gif' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="">
                <papertitle>Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning</papertitle>
                </a>
                <br>
                <strong>Yongyuan Liang*</strong>, Yanchao Sun*, Ruijie Zheng, Furong Huang
                <br>
                <br>
                <em><strong>NeurIPS</strong></em>, 2022
                <br>
                <em>NeurIPS Workshop SafeRL</em>, 2021 <font color=#C21232><strong>(Spotlight Talks)</strong></font></div>
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8d6b1d775014eff18256abeb207202ad-Paper-Conference.pdf">Paper</a> &nbsp/&nbsp
                <a href="https://github.com/umd-huang-lab/WocaR-RL">Code</a>&nbsp/&nbsp
                <a href="https://neurips.cc/media/neurips-2022/Slides/54214.pdf">Slides</a>
                </p>
                </td>
                </tr>

                <tr>
                  <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                  <img src='images/backdoor.png' width="230">
                  </td>
                  <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                  <p>
                  <a href="https://openreview.net/attachment?id=QTviVh3VQU&name=pdf">
                  <papertitle>Is poisoning a real threat to LLM alignment? Maybe more so than you think</papertitle>
                  </a>
                  <br>
                  Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, <strong>Yongyuan Liang</strong>, Furong Huang
                  <br>				
                  <br>
                  <em><strong>AAAI</strong></em>, 2025</div><br>
                  <em>ICML Workshop on Models of Human Feedback for AI Alignment</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2406.12091">Paper</a> &nbsp/&nbsp
                  <a href="">Code</a>
                  </p>
                  </td>
                </tr>

                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images//beyond.png' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="https://protected-beyond-worst-case.github.io">
                <papertitle>Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies</papertitle>
                </a>
                <br>
                Xiangyu Liu*, Chenghao Deng*, Yanchao Sun, <strong>Yongyuan Liang</strong>, Furong Huang
                <br>
                <br>
                <em><strong>ICLR</strong></em>, 2024 <font color=#C21232><strong>(Spotlight - Top 5%)</strong></font>
                <br>
                <em>NeurIPS Workshop MASEC</em>, 2023
                <br>
                <a href="https://protected-beyond-worst-case.github.io/home/">Project Page</a>&nbsp/&nbsp
                <a href="https://openreview.net/pdf?id=DFTHW0MyiW">Paper</a>&nbsp/&nbsp
                <a href="https://github.com/umd-huang-lab/PROTECTED">Code</a>&nbsp/&nbsp
                <a href="https://x.com/furongh/status/1760859987851591681?s=20">Twitter</a>
                </p>
                </td>
                </tr>

                <tr>
                <td class="tdimg" style="padding:20px;width:30%;vertical-align:center">
                <img src='images/paad.gif' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:70%;vertical-align:center">
                <p>
                <a href="https://ycsun2017.github.io/paad/index.html">
                <papertitle>Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL</papertitle>
                </a>
                <br>
                Yanchao Sun, Ruijie Zheng, <strong>Yongyuan Liang</strong>, Furong Huang
                <br>
                <br>
                <em><strong>ICLR</strong></em>, 2022
                <br>
                <em>NeurIPS Workshop SafeRL</em>, 2021 <font color=#C21232><strong>(Best Paper Award)</strong></font>
                <br>
                <a href="https://umd-huang-lab.github.io/evasion-rl/">Project Page</a>&nbsp/&nbsp
                <a href="https://openreview.net/pdf?id=JM2kFbJvvI">Paper</a>&nbsp/&nbsp
                <a href="https://github.com/umd-huang-lab/paad_adv_rl">Code</a>
                </p>
                </td>
                </tr>

                <tr>
                <td class="tdimg" style="padding:20px;width:25%;vertical-align:center">
                <img src='images/cmarl.png' width="230">
                </td>
                <td class="tdcontent" style="padding:20px;width:75%;vertical-align:center">
                <p>
                <a href="https://openreview.net/forum?id=dCOL0inGl3e">
                <papertitle>Certifiably Robust Policy Learning against Adversarial Communication in Multi-agent Systems</papertitle>
                </a>
                <br>
                Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, <strong>Yongyuan Liang</strong>, Soheil Feizi, Sumitra Ganesh, Furong Huang
                <br>
                <br>
                <em><strong>ICLR</strong></em>, 2023
                <br>
                <a href="https://openreview.net/pdf?id=dCOL0inGl3e">Paper</a> &nbsp/&nbsp
                <a href="">Code</a>
                </p>
                </td>
                </tr>

                </tbody></table>
                <br>

<heading>Professional Service</heading>
<table style="padding:0px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr>
    <td>
      <p><strong>Conference Program Committee:</strong> ICML(2022, 2023, 2024, 2025), NeurIPS(2021, 2022, 2023, 2024, 2025), ICLR(2021, 2022, 2023, 2024, 2025)</p>
    </td>
  </tr>
  <tr>
    <td>
      <p><strong>Workshop Program Committee:</strong> <a href="https://sites.google.com/view/fmdm-neurips23/">FMDM at NeurIPS 2023</a>, <a href="https://bialign-workshop.github.io/">Bi-Align at ICLR 2025</a>, <a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">CVinW at CVPR 2025</a></p>
    </td>
  </tr>
  <br>
  <br>
</tbody></table>
<br>

<br>
<p></p>
<misc><b>Misc</b></misc>
<br>
<br>
<table style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <p>If my name is a bit tricky to pronounce for you, it is also great to call me Cheryl [ˈʃerəl].</p>
  <p>I've been playing the violin🎻 for over 15 years and served as a principal violinist in the university orchestra.</p>
  <p>Been a fan of Novak Djokovic since 2012.</p>

  <p>My Erdős number = <a href="https://www.csauthors.net/distance/paul-erdos/yongyuan-liang">4</a>.</p>
  </tr>
</tbody></table>
                
<br>
<br>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <br>
      <p style="text-align:right;font-size:small;color:#a0a7b38b">&copy; Yongyuan Liang<br><a style="font-size:small;color:#a0a7b38b" href="https://github.com/jonbarron/website">credits</a></font></p>
    </td>
  </tr>
</tbody></table>

</body>

</html>
